{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN_turb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNswusw580E1gxeTGmbKjSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YKulki/Turbulence-Modelling/blob/main/FCN_turb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t23paOdV6m9T",
        "outputId": "2540cf97-586f-4cdf-c921-bab535e50b81"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWAWmWCJ7kCe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCnSvH7m7wGX",
        "outputId": "c26e7b9d-8dd0-4596-9bb2-57d47a26726e"
      },
      "source": [
        "datapath='/content/drive/MyDrive/Turbulence_data/data.npy'\n",
        "dataset=np.load(datapath)\n",
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1046, 600, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io337-Rp7x5k",
        "outputId": "5e270720-e4f9-484a-ad6d-bec33a82f0b6"
      },
      "source": [
        "train=dataset[:,0:400,:]\n",
        "test=dataset[:,401:599,:]\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1046, 400, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spl2VWgg7cmV"
      },
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, x, y, t, u, v, layers):\n",
        "        \n",
        "        X = np.concatenate([x, y, t], 1)\n",
        "        \n",
        "        '''\n",
        "        self.lb = X.min(0)\n",
        "        self.ub = X.max(0)'''\n",
        "                \n",
        "        self.X = X\n",
        "        \n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.t = t\n",
        "        \n",
        "        self.u = u\n",
        "        self.v = v\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NN\n",
        "        self.weights, self.biases = self.initialize_NN(layers)        \n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        self.lambda_2 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
        "        \n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
        "        self.y_tf = tf.placeholder(tf.float32, shape=[None, self.y.shape[1]])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
        "        \n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
        "        self.v_tf = tf.placeholder(tf.float32, shape=[None, self.v.shape[1]])\n",
        "        \n",
        "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred = self.net_NS(self.x_tf, self.y_tf, self.t_tf)\n",
        "        \n",
        "        self.loss = tf.reduce_sum(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.v_tf - self.v_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_v_pred))\n",
        "                    \n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
        "                                                                method = 'L-BFGS-B', \n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})        \n",
        "        \n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)                    \n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "        \n",
        "    def net_NS(self, x, y, t):\n",
        "        lambda_1 = self.lambda_1\n",
        "        lambda_2 = self.lambda_2\n",
        "        \n",
        "        psi_and_p = self.neural_net(tf.concat([x,y,t], 1), self.weights, self.biases)\n",
        "        psi = psi_and_p[:,0:1]\n",
        "        p = psi_and_p[:,1:2]\n",
        "        \n",
        "        u = tf.gradients(psi, y)[0]\n",
        "        v = -tf.gradients(psi, x)[0]  \n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_y = tf.gradients(u, y)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        u_yy = tf.gradients(u_y, y)[0]\n",
        "        \n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "        v_y = tf.gradients(v, y)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "        v_yy = tf.gradients(v_y, y)[0]\n",
        "        \n",
        "        p_x = tf.gradients(p, x)[0]\n",
        "        p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
        "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy)\n",
        "        \n",
        "        return u, v, p, f_u, f_v\n",
        "    \n",
        "    def callback(self, loss, lambda_1, lambda_2):\n",
        "        print('Loss: %.3e, l1: %.3f, l2: %.5f' % (loss, lambda_1, lambda_2))\n",
        "      \n",
        "    def train(self, nIter): \n",
        "\n",
        "        tf_dict = {self.x_tf: self.x, self.y_tf: self.y, self.t_tf: self.t,\n",
        "                   self.u_tf: self.u, self.v_tf: self.v}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                lambda_1_value = self.sess.run(self.lambda_1)\n",
        "                lambda_2_value = self.sess.run(self.lambda_2)\n",
        "                print('It: %d, Loss: %.3e, l1: %.3f, l2: %.5f, Time: %.2f' % \n",
        "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
        "                start_time = time.time()\n",
        "            \n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict = tf_dict,\n",
        "                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
        "                                loss_callback = self.callback)\n",
        "            \n",
        "    \n",
        "    def predict(self, x_star, y_star, t_star):\n",
        "        \n",
        "        tf_dict = {self.x_tf: x_star, self.y_tf: y_star, self.t_tf: t_star}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
        "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
        "        \n",
        "        return u_star, v_star, p_star\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKQBS6Br7uxr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}